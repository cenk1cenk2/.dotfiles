#!/usr/bin/env python3

import argparse
import json
import logging
import os
import re
import subprocess
import sys
import tempfile
from contextlib import contextmanager
from dataclasses import dataclass, field
from enum import StrEnum
from pathlib import Path

log = logging.getLogger("remsi")

class RegionKind(StrEnum):
    SILENCE = "silence"
    FILLER = "filler"
    GAP = "gap"
    SPEECH = "speech"

    @property
    def priority(self):
        return {self.SILENCE: 2, self.FILLER: 1, self.GAP: 0, self.SPEECH: -1}[self]

    def color(self):
        return {
            self.SILENCE: YELLOW,
            self.FILLER: RED,
            self.GAP: DIM,
            self.SPEECH: GREEN,
        }[self]

@dataclass
class Region:
    start: float
    end: float
    kind: RegionKind

@dataclass
class Segment:
    start: float
    end: float
    left: Region | None = field(default=None, repr=False)
    right: Region | None = field(default=None, repr=False)

@dataclass
class VideoInfo:
    codec: str | None = None
    bitrate: int | None = None
    width: int | None = None
    height: int | None = None
    fps: float | None = None
    pix_fmt: str | None = None
    color_space: str | None = None
    color_transfer: str | None = None
    color_primaries: str | None = None
    color_range: str | None = None

    def __str__(self):
        parts = []
        if self.codec:
            parts.append(self.codec)
        if self.width and self.height:
            parts.append(f"{self.width}x{self.height}")
        if self.fps:
            parts.append(f"{self.fps:g}fps")
        if self.pix_fmt:
            parts.append(self.pix_fmt)
        if self.color_space and self.color_space != "unknown":
            parts.append(self.color_space)
        if self.color_range and self.color_range != "unknown":
            parts.append(self.color_range)
        if self.bitrate:
            parts.append(f"{self.bitrate // 1000}k")

        return " ".join(parts) or "?"

@dataclass
class AudioInfo:
    codec: str | None = None
    bitrate: int | None = None
    sample_rate: int | None = None
    channels: int | None = None

    def __str__(self):
        parts = []
        if self.codec:
            parts.append(self.codec)
        if self.sample_rate:
            parts.append(f"{self.sample_rate // 1000}kHz")
        if self.channels:
            parts.append(f"{self.channels}ch")
        if self.bitrate:
            parts.append(f"{self.bitrate // 1000}k")

        return " ".join(parts) or "?"

@dataclass
class MediaInfo:
    video: VideoInfo = field(default_factory=VideoInfo)
    audio: AudioInfo = field(default_factory=AudioInfo)
    size: int | None = None

    @property
    def size_str(self):
        if self.size is None:
            return "?"
        if self.size >= 1_073_741_824:
            return f"{self.size / 1_073_741_824:.2f} GB"
        if self.size >= 1_048_576:
            return f"{self.size / 1_048_576:.1f} MB"
        if self.size >= 1024:
            return f"{self.size / 1024:.1f} KB"

        return f"{self.size} B"

_use_color = sys.stderr.isatty()

def _ansi(code):
    if _use_color:
        return f"\033[{code}m"

    return ""

RESET = _ansi(0)
BOLD = _ansi(1)
DIM = _ansi(2)
GREEN = _ansi(32)
YELLOW = _ansi(33)
RED = _ansi(31)

def _term_width():
    try:
        return os.get_terminal_size().columns
    except OSError:
        return 80

def section(label):
    w = _term_width()
    pad = max(0, w - len(label) - 2)
    left = pad // 2
    right = pad - left
    line = f"{DIM}{'─' * left}{RESET}{BOLD}{YELLOW} {label} {RESET}{DIM}{'─' * right}{RESET}"
    print(f"\n{line}", file=sys.stderr)

def status(msg):
    print(msg, file=sys.stderr)

def error(msg):
    print(f"{RED}error:{RESET} {msg}", file=sys.stderr)

def format_timestamp(seconds):
    s = float(seconds)
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)

    return f"{int(h):02d}:{int(m):02d}:{s:06.3f}"

FILLER_PATTERN = re.compile(
    r"^("
    r"u+[hm]+|"  # uh, uhh, um, umm, uhm
    r"[hm]+m*|"  # hm, hmm, mm, mmm, mhm
    r"e+r+m*|"  # er, erm, errr
    r"a+h*|"  # ah, ahh, aaa, aaah
    r"o+h+|"  # oh, ohh
    r"e+h+|"  # eh, ehh
    r")$"
)

class Analyzer:
    def __init__(self, noise, duration, model_path=None):
        self.noise = noise
        self.duration = duration
        self.model_path = model_path

    def get_duration(self, input_file):
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-show_entries",
                "format=duration",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                str(input_file),
            ],
            capture_output=True,
            text=True,
        )
        try:
            return float(result.stdout.strip())
        except ValueError:
            return None

    def detect_silence(self, input_file, total):
        cmd = [
            "ffmpeg",
            "-i",
            str(input_file),
            "-hide_banner",
            "-af",
            f"silencedetect=n={self.noise}:d={self.duration}",
            "-f",
            "null",
            "-",
        ]
        log.debug("silence detect cmd: %s", " ".join(cmd))
        proc = subprocess.Popen(
            cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True
        )
        lines = []
        for line in proc.stderr:
            sys.stderr.write(line)
            lines.append(line)
        proc.wait()
        if proc.returncode != 0:
            raise RuntimeError(f"silence detection failed (exit {proc.returncode})")
        output = "".join(lines)

        silences = []
        silence_start = None
        for line in output.splitlines():
            start_match = re.search(r"silence_start: (\d+\.?\d+)", line)
            end_match = re.search(r"silence_end: (\d+\.?\d+)", line)
            if start_match:
                silence_start = float(start_match.group(1))
            if end_match:
                if silence_start is not None:
                    silences.append(
                        Region(
                            silence_start, float(end_match.group(1)), RegionKind.SILENCE
                        )
                    )
                    silence_start = None
        if silence_start is not None:
            silences.append(Region(silence_start, total, RegionKind.SILENCE))

        return silences

    def _transcribe(self, input_file):
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
            extract = subprocess.run(
                [
                    "ffmpeg",
                    "-i",
                    str(input_file),
                    "-ar",
                    "16000",
                    "-ac",
                    "1",
                    "-f",
                    "wav",
                    "-y",
                    tmp.name,
                ],
                capture_output=True,
            )
            if extract.returncode != 0:
                raise RuntimeError("failed to extract audio for transcription")
            log.info("transcribing %s", input_file)
            result = subprocess.run(
                [
                    "whisper-cli",
                    "-m",
                    str(self.model_path),
                    "-pp",
                    "--max-len",
                    "1",
                    "--split-on-word",
                    "-oj",
                    "-of",
                    tmp.name,
                    tmp.name,
                ],
                stdout=subprocess.DEVNULL,
            )
            if result.returncode != 0:
                raise RuntimeError(f"whisper-cli failed (exit {result.returncode})")

            json_path = f"{tmp.name}.json"
            try:
                with open(json_path) as f:
                    return json.load(f)
            finally:
                Path(json_path).unlink(missing_ok=True)

    @staticmethod
    def _classify_words(transcription):
        speech = []
        fillers = []
        for seg in transcription.get("transcription", []):
            text = seg["text"].strip()
            letters = re.sub(r"[^a-z]", "", text.lower())
            start = seg["offsets"]["from"] / 1000
            end = seg["offsets"]["to"] / 1000
            if start >= end:
                continue
            is_filler = (not text) or (letters and FILLER_PATTERN.match(letters))
            if is_filler:
                fillers.append(Region(start, end, RegionKind.FILLER))
                log.debug(
                    "  transcribed filler: '%s' at %s -> %s",
                    text,
                    format_timestamp(start),
                    format_timestamp(end),
                )
            else:
                speech.append(Region(start, end, RegionKind.SPEECH))

        return speech, fillers

    @staticmethod
    def _find_uncovered_gaps(known_regions):
        known_regions.sort(key=lambda r: r.start)
        merged = []
        for r in known_regions:
            if merged and r.start <= merged[-1].end:
                merged[-1] = Region(
                    merged[-1].start, max(merged[-1].end, r.end), merged[-1].kind
                )
            else:
                merged.append(r)

        gaps = []
        pos = 0.0
        for r in merged:
            if r.start > pos:
                gaps.append(Region(pos, r.start, RegionKind.GAP))
            pos = r.end

        return gaps

    def detect_filler_words(self, input_file, silences):
        data = self._transcribe(input_file)
        if data is None:
            return []

        speech, fillers = self._classify_words(data)
        gaps = self._find_uncovered_gaps(silences + speech + fillers)
        fillers.extend(gaps)

        for i, r in enumerate(fillers, 1):
            log.info(
                "  %d. %s -> %s (%.1fs) %s",
                i,
                format_timestamp(r.start),
                format_timestamp(r.end),
                r.end - r.start,
                r.kind,
            )
        log.info("speech segments: %d, filler gaps: %d", len(speech), len(fillers))

        return fillers

    @staticmethod
    def merge_regions(silences, fillers):
        regions = list(silences) + list(fillers)
        regions.sort(key=lambda r: r.start)

        if not regions:
            return []

        merged = [regions[0]]
        for r in regions[1:]:
            prev = merged[-1]
            if r.start <= prev.end:
                stronger = (
                    prev.kind if prev.kind.priority >= r.kind.priority else r.kind
                )
                merged[-1] = Region(prev.start, max(prev.end, r.end), stronger)
            else:
                merged.append(r)

        return merged

    @staticmethod
    def regions_to_segments(regions, total):
        segments = []
        pos = 0.0
        for region in regions:
            if region.start > pos:
                left = segments[-1].right if segments else None
                segments.append(
                    Segment(start=pos, end=region.start, left=left, right=region)
                )
            pos = region.end
        if pos < total:
            left = segments[-1].right if segments else None
            segments.append(Segment(start=pos, end=total, left=left))

        return segments

class Encoder:
    GPU_ENCODERS = {
        "h264": {"nvidia": "h264_nvenc", "amd": "h264_amf", "vaapi": "h264_vaapi"},
        "hevc": {"nvidia": "hevc_nvenc", "amd": "hevc_amf", "vaapi": "hevc_vaapi"},
        "av1": {"nvidia": "av1_nvenc", "amd": "av1_amf", "vaapi": "av1_vaapi"},
    }

    def __init__(
        self, gpu, codec, fade_time, video_filter, audio_filter, force=False, fast=False
    ):
        self.gpu = gpu
        self.codec = codec
        self.fade_time = fade_time
        self.video_filter = video_filter
        self.audio_filter = audio_filter
        self.force = force
        self.fast = fast

    @staticmethod
    def detect_gpu():
        encoders = subprocess.run(
            ["ffmpeg", "-hide_banner", "-encoders"],
            capture_output=True,
            text=True,
        ).stdout

        if "hevc_nvenc" in encoders:
            return "nvidia"
        if "hevc_amf" in encoders:
            return "amd"
        if "hevc_vaapi" in encoders:
            return "vaapi"

        return None

    @staticmethod
    def _ffprobe_stream(input_file, stream_type, fields):
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                f"{stream_type}:0",
                "-show_entries",
                f"stream={','.join(fields)}",
                "-of",
                "json",
                str(input_file),
            ],
            capture_output=True,
            text=True,
        )
        try:
            streams = json.loads(result.stdout).get("streams", [])
            if streams:
                return streams[0]
        except (json.JSONDecodeError, IndexError):
            pass

        return {}

    @staticmethod
    def _parse_fps(r_frame_rate):
        try:
            num, den = r_frame_rate.split("/")

            return round(int(num) / int(den), 3)
        except (ValueError, ZeroDivisionError, AttributeError):
            return None

    @staticmethod
    def probe(input_file):
        vs = Encoder._ffprobe_stream(
            input_file,
            "v",
            [
                "codec_name",
                "bit_rate",
                "width",
                "height",
                "r_frame_rate",
                "pix_fmt",
                "color_space",
                "color_transfer",
                "color_primaries",
                "color_range",
            ],
        )
        a = Encoder._ffprobe_stream(
            input_file,
            "a",
            ["codec_name", "bit_rate", "sample_rate", "channels"],
        )

        def _int(v):
            try:
                return int(v) if v else None
            except (ValueError, TypeError):
                return None

        try:
            size = Path(input_file).stat().st_size
        except OSError:
            size = None

        return MediaInfo(
            size=size,
            video=VideoInfo(
                codec=vs.get("codec_name"),
                bitrate=_int(vs.get("bit_rate")),
                width=_int(vs.get("width")),
                height=_int(vs.get("height")),
                fps=Encoder._parse_fps(vs.get("r_frame_rate")),
                pix_fmt=vs.get("pix_fmt"),
                color_space=vs.get("color_space"),
                color_transfer=vs.get("color_transfer"),
                color_primaries=vs.get("color_primaries"),
                color_range=vs.get("color_range"),
            ),
            audio=AudioInfo(
                codec=a.get("codec_name"),
                bitrate=_int(a.get("bit_rate")),
                sample_rate=_int(a.get("sample_rate")),
                channels=_int(a.get("channels")),
            ),
        )

    def _video_codec_args(self, info):
        if self.codec:
            return ["-c:v", self.codec]

        family = None
        if info.codec:
            name = info.codec.lower()
            if name in ("h264", "libx264"):
                family = "h264"
            elif name in ("hevc", "h265", "libx265"):
                family = "hevc"
            elif name in ("av1", "libsvtav1", "libaom-av1"):
                family = "av1"

        args = []
        if self.gpu and family and family in self.GPU_ENCODERS:
            hw_enc = self.GPU_ENCODERS[family].get(self.gpu)
            if hw_enc:
                if self.gpu == "vaapi":
                    args = [
                        "-vaapi_device",
                        "/dev/dri/renderD128",
                        "-c:v",
                        hw_enc,
                        "-qp",
                        "20",
                    ]
                elif self.gpu == "nvidia":
                    args = [
                        "-c:v",
                        hw_enc,
                        "-preset",
                        "p5",
                        "-tune",
                        "hq",
                        "-rc",
                        "constqp",
                        "-qp",
                        "20",
                        "-multipass",
                        "qres",
                        "-bf",
                        "2",
                    ]
                else:
                    args = ["-c:v", hw_enc, "-qp", "20"]
        elif info.codec:
            args = ["-c:v", info.codec, "-crf", "20"]

        if not args:
            return []
        if info.pix_fmt:
            args.extend(["-pix_fmt", info.pix_fmt])
        if info.color_space and info.color_space != "unknown":
            args.extend(["-colorspace", info.color_space])
        if info.color_transfer and info.color_transfer != "unknown":
            args.extend(["-color_trc", info.color_transfer])
        if info.color_primaries and info.color_primaries != "unknown":
            args.extend(["-color_primaries", info.color_primaries])
        if info.color_range and info.color_range != "unknown":
            cr = (
                "mpeg"
                if info.color_range == "tv"
                else "jpeg"
                if info.color_range == "pc"
                else info.color_range
            )
            args.extend(["-color_range", cr])

        return args

    def _audio_codec_args(self, info):
        args = ["-c:a", info.codec or "aac"]
        if info.bitrate:
            args.extend(["-b:a", str(info.bitrate)])

        return args

    def _xfade_expr(self, offset):
        name, _, extra = self.video_filter.partition(":")
        parts = [f"{name}=offset={offset}:duration={self.fade_time}"]
        if extra:
            parts.append(extra)

        return ":".join(parts)

    def _acrossfade_expr(self):
        name, _, extra = self.audio_filter.partition(":")
        parts = [f"{name}=duration={self.fade_time}"]
        if extra:
            parts.append(extra)

        return ":".join(parts)

    @staticmethod
    @contextmanager
    def _filter_script(lines):
        content = ";\n".join(lines)
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".txt", prefix="remsi_filter_", delete=True
        ) as f:
            f.write(content)
            f.flush()
            log.debug("filter_complex_script %s:\n%s", f.name, content)

            yield f.name

    def _should_crossfade(self, i, segments):
        return (
            self.fade_time > 0
            and i < len(segments) - 1
            and segments[i].right is not None
            and segments[i].right.kind == RegionKind.SILENCE
            and (segments[i].end - segments[i].start) >= self.fade_time
            and (segments[i + 1].end - segments[i + 1].start) >= self.fade_time
        )

    def _build_filter_lines(self, segments, video_info=None):
        n = len(segments)

        color_filters = ""
        if video_info:
            parts = []
            if video_info.pix_fmt:
                parts.append(f"format={video_info.pix_fmt}")
            sp = []
            if video_info.color_space and video_info.color_space != "unknown":
                sp.append(f"colorspace={video_info.color_space}")
            if video_info.color_transfer and video_info.color_transfer != "unknown":
                sp.append(f"color_trc={video_info.color_transfer}")
            if video_info.color_primaries and video_info.color_primaries != "unknown":
                sp.append(f"color_primaries={video_info.color_primaries}")
            if video_info.color_range and video_info.color_range != "unknown":
                r = (
                    "limited"
                    if video_info.color_range == "tv"
                    else "full"
                    if video_info.color_range == "pc"
                    else video_info.color_range
                )
                sp.append(f"range={r}")
            if sp:
                parts.append("setparams=" + ":".join(sp))
            if parts:
                color_filters = "," + ",".join(parts)

        lines = []
        for i, seg in enumerate(segments):
            lines.append(
                f"[0:v]trim={seg.start}:{seg.end},setpts=PTS-STARTPTS,settb=AVTB{color_filters}[v{i}]"
            )
            lines.append(f"[0:a]atrim={seg.start}:{seg.end},asetpts=PTS-STARTPTS[a{i}]")

        if self.fade_time > 0 and n > 1:
            accumulated = 0.0
            xfade_count = 0
            v_label = "[v0]"
            for i in range(n - 1):
                accumulated += segments[i].end - segments[i].start
                if self._should_crossfade(i, segments):
                    offset = accumulated - (xfade_count + 1) * self.fade_time
                    out = f"[xf{xfade_count}]"
                    lines.append(f"{v_label}[v{i + 1}]{self._xfade_expr(offset)}{out}")
                    v_label = out
                    xfade_count += 1
                else:
                    out = f"[vc{i}]"
                    lines.append(f"{v_label}[v{i + 1}]concat=n=2:v=1:a=0{out}")
                    v_label = out
            lines.append(f"{v_label}null[vout]")
        else:
            vf_labels = "".join(f"[v{i}]" for i in range(n))
            lines.append(f"{vf_labels}concat=n={n}:v=1:a=0[vout]")

        if self.fade_time > 0 and n > 1:
            a_label = "[a0]"
            for i in range(n - 1):
                if self._should_crossfade(i, segments):
                    out = f"[ax{i}]"
                    lines.append(f"{a_label}[a{i + 1}]{self._acrossfade_expr()}{out}")
                    a_label = out
                else:
                    out = f"[ac{i}]"
                    lines.append(f"{a_label}[a{i + 1}]concat=n=2:v=0:a=1{out}")
                    a_label = out
            lines.append(f"{a_label}anull[aout]")
        else:
            af_labels = "".join(f"[a{i}]" for i in range(n))
            lines.append(f"{af_labels}concat=n={n}:v=0:a=1[aout]")

        return lines

    @staticmethod
    def _run(cmd):
        log.debug("encode cmd: %s", " ".join(cmd))

        return subprocess.run(cmd)

    def _encode_fast(self, input_file, output_file, segments):
        with tempfile.TemporaryDirectory(prefix="remsi_fast_") as tmpdir:
            tmpdir = Path(tmpdir)
            part_files = []
            for i, seg in enumerate(segments):
                part = tmpdir / f"part{i:04d}{input_file.suffix}"
                cmd = [
                    "ffmpeg",
                    "-hide_banner",
                    "-loglevel",
                    "warning",
                    "-ss",
                    str(seg.start),
                    "-to",
                    str(seg.end),
                    "-i",
                    str(input_file),
                    "-c",
                    "copy",
                    "-avoid_negative_ts",
                    "make_zero",
                    "-y",
                    str(part),
                ]
                log.debug("fast split cmd: %s", " ".join(cmd))
                result = subprocess.run(cmd)
                if result.returncode != 0:
                    return result
                part_files.append(part)

            concat_list = tmpdir / "concat.txt"
            with open(concat_list, "w") as f:
                for part in part_files:
                    f.write(f"file '{part}'\n")

            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(concat_list),
                "-c",
                "copy",
            ]
            if self.force:
                cmd.append("-y")
            cmd.append(str(output_file))

            return self._run(cmd)

    def encode(self, input_file, output_file, segments, media=None):
        n = len(segments)
        media = media or MediaInfo()

        if n == 1:
            seg = segments[0]
            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-ss",
                str(seg.start),
                "-to",
                str(seg.end),
                "-i",
                str(input_file),
                "-c:v",
                "copy",
                "-c:a",
                "copy",
            ]
            if self.force:
                cmd.append("-y")
            cmd.append(str(output_file))

            return self._run(cmd)

        if self.fast:
            return self._encode_fast(input_file, output_file, segments)

        lines = self._build_filter_lines(segments, media.video)
        with self._filter_script(lines) as script_path:
            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-i",
                str(input_file),
                "-/filter_complex",
                script_path,
                "-map",
                "[vout]",
                "-map",
                "[aout]",
            ]
            if self.force:
                cmd.append("-y")
            cmd.extend(self._video_codec_args(media.video))
            cmd.extend(self._audio_codec_args(media.audio))
            cmd.append(str(output_file))

            return self._run(cmd)

class Remsi:
    def __init__(self, args):
        self.args = args
        self.analyzer = Analyzer(
            noise=args.noise,
            duration=args.duration,
            model_path=self._resolve_model_path(),
        )
        self.encoder = Encoder(
            gpu=self._resolve_gpu(),
            codec=args.codec,
            fade_time=args.fade_time,
            video_filter=args.fade_video_filter,
            audio_filter=args.fade_audio_filter,
            force=args.force,
            fast=args.fast,
        )

    def _resolve_gpu(self):
        if self.args.gpu == "auto":
            gpu = Encoder.detect_gpu()
            if gpu:
                log.info("detected gpu encoder: %s", gpu)
                status(f"GPU encoder: {GREEN}{gpu}{RESET}")
            else:
                log.warning("no gpu encoder found, using software encoding")

            return gpu
        if self.args.gpu != "none":
            return self.args.gpu

        return None

    def _resolve_model_path(self):
        if not self.args.whisper:
            return None

        model_path = Path(self.args.model_dir).expanduser() / self.args.model
        if not model_path.exists():
            error(f"model not found at {model_path}")
            sys.exit(1)

        return model_path

    def _process_file(self, input_file, output_file):
        section(f"Processing {input_file.name}")

        total = self.analyzer.get_duration(input_file)
        if total is None:
            error(f"could not determine duration of {input_file}")

            return
        media = Encoder.probe(input_file)
        status(f"duration:\t{BOLD}{format_timestamp(total)}{RESET}")
        status(f"size:\t\t{YELLOW}{media.size_str}{RESET}")
        status(f"video:\t\t{YELLOW}{media.video}{RESET}")
        status(f"audio:\t\t{YELLOW}{media.audio}{RESET}")
        log.debug(
            "silence params: noise=%s duration=%s", self.args.noise, self.args.duration
        )

        try:
            section("Silence Detection")
            silences = self.analyzer.detect_silence(input_file, total)
            status(f"found {YELLOW}{len(silences)}{RESET} silent region(s)")

            fillers = []
            if self.analyzer.model_path:
                section("Filler Word Detection")
                fillers = self.analyzer.detect_filler_words(input_file, silences)
                status(f"found {YELLOW}{len(fillers)}{RESET} filler region(s)")
        except RuntimeError as e:
            error(str(e))

            return

        all_regions = Analyzer.merge_regions(silences, fillers)
        min_cut = self.args.min_cut
        if min_cut > 0:
            skipped = [r for r in all_regions if (r.end - r.start) < min_cut]
            cut = [r for r in all_regions if (r.end - r.start) >= min_cut]
            if skipped:
                status(
                    f"skipping {YELLOW}{len(skipped)}{RESET} region(s) "
                    f"shorter than {min_cut}s"
                )
            all_regions = cut
        segments = self.analyzer.regions_to_segments(all_regions, total)

        if not all_regions:
            status(f"{DIM}nothing to remove, skipping{RESET}")

            return

        kept = sum(seg.end - seg.start for seg in segments)
        removed = total - kept
        pct = (removed / total * 100) if total > 0 else 0
        status(
            f"removing {YELLOW}{removed:.1f}s{RESET} of {total:.1f}s "
            f"({BOLD}{pct:.1f}%{RESET})"
        )

        section("Summary")
        timeline = [
            Region(seg.start, seg.end, RegionKind.SPEECH) for seg in segments
        ] + list(all_regions)
        timeline.sort(key=lambda r: r.start)
        for i, r in enumerate(timeline, 1):
            status(
                f"{DIM}{i:3d}.{RESET} {format_timestamp(r.start)} -> "
                f"{format_timestamp(r.end)} {DIM}({r.end - r.start:.1f}s){RESET} "
                f"{r.kind.color()}{r.kind}{RESET}"
            )

        section("Encoding")
        status(f"writing {GREEN}{output_file}{RESET}")
        try:
            result = self.encoder.encode(input_file, output_file, segments, media)
        except KeyboardInterrupt:
            error("interrupted, cleaning up...")
            sys.exit(130)

        if result.returncode != 0:
            error(f"ffmpeg exited with code {result.returncode}")

            return

        section(f"{input_file.name} -> {output_file.stem}")
        parts = []
        if silences:
            parts.append(f"{YELLOW}{len(silences)}{RESET} silence(s)")
        if fillers:
            parts.append(f"{YELLOW}{len(fillers)}{RESET} filler(s)")
        if parts:
            status(f"detected {' and '.join(parts)}")
        status(
            f"trimmed {YELLOW}{removed:.1f}s{RESET} ({BOLD}{pct:.1f}%{RESET}), "
            f"{format_timestamp(total)} down to {GREEN}{format_timestamp(kept)}{RESET}"
        )
        out_media = Encoder.probe(output_file)
        status(f"size:\t\t{YELLOW}{out_media.size_str}{RESET}")
        status(f"video:\t\t{YELLOW}{out_media.video}{RESET}")
        status(f"audio:\t\t{YELLOW}{out_media.audio}{RESET}")

        status(f"\n{GREEN}{output_file}{RESET}")

    def run(self):
        for input_path in self.args.input:
            input_file = Path(input_path)
            if not input_file.exists():
                error(f"{input_file} not found")
                continue

            if self.args.output:
                output_file = Path(self.args.output)
            else:
                output_file = input_file.with_stem(
                    f"{input_file.stem}-{self.args.suffix}"
                )

            self._process_file(input_file, output_file)

def main():
    parser = argparse.ArgumentParser(
        description="Remove silent parts from video files using FFmpeg.",
    )
    parser.add_argument("input", nargs="+", help="input video file(s)")
    parser.add_argument("-o", "--output", help="output file, only with single input")
    parser.add_argument("-n", "--noise", default="-45dB", help="silence threshold")
    parser.add_argument(
        "-d",
        "--duration",
        type=float,
        default=0.8,
        help="minimum silence duration in seconds",
    )
    parser.add_argument(
        "--min-cut",
        type=float,
        default=0.05,
        help="minimum region duration to cut (smaller regions are kept)",
    )
    parser.add_argument(
        "--gpu",
        choices=["nvidia", "amd", "vaapi", "auto", "none"],
        default="auto",
        help="GPU acceleration",
    )
    parser.add_argument("--codec", help="override video codec")
    parser.add_argument(
        "--fade-time",
        type=float,
        default=0.1,
        help="crossfade duration in seconds (0 to disable)",
    )
    parser.add_argument(
        "--fade-video-filter",
        default="xfade:transition=fadefast",
        help="video crossfade filter spec (name[:extra_params])",
    )
    parser.add_argument(
        "--fade-audio-filter",
        default="acrossfade:curve1=log:curve2=log",
        help="audio crossfade filter spec (name[:extra_params])",
    )
    parser.add_argument("--suffix", default="silencer", help="output filename suffix")
    parser.add_argument(
        "--whisper",
        action="store_true",
        help="also detect filler words using speech recognition",
    )
    parser.add_argument(
        "--model-dir",
        default="~/.local/share/applications/waystt/models",
        help="directory containing whisper GGML models",
    )
    parser.add_argument(
        "--model",
        default="ggml-large-v3.bin",
        help="whisper GGML model filename",
    )
    parser.add_argument(
        "--fast",
        action="store_true",
        help="use stream copy with concat demuxer (no re-encoding, no crossfade)",
    )
    parser.add_argument(
        "-f", "--force", action="store_true", help="overwrite output file if it exists"
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="verbose logging")
    args = parser.parse_args()

    logging.basicConfig(
        format="%(name)s: %(message)s",
        level=logging.DEBUG if args.verbose else logging.WARNING,
    )

    if args.output and len(args.input) > 1:
        parser.error("-o/--output can only be used with a single input file")

    Remsi(args).run()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nInterrupted", file=sys.stderr)
        sys.exit(130)
