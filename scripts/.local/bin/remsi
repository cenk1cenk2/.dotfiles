#!/usr/bin/env python3

import argparse
import bisect
import json
import logging
import os
import re
import subprocess
import sys
import tempfile
import time
from contextlib import contextmanager
from dataclasses import dataclass, field
from enum import StrEnum
from pathlib import Path

log = logging.getLogger("remsi")

class RegionKind(StrEnum):
    SILENCE = "silence"
    FILLER = "filler"
    STUTTER = "stutter"
    GAP = "gap"
    SPEECH = "speech"

    @property
    def priority(self):
        return {
            self.SILENCE: 2,
            self.FILLER: 1,
            self.STUTTER: 1,
            self.GAP: 0,
            self.SPEECH: -1,
        }[self]

    def color(self):
        return {
            self.SILENCE: YELLOW,
            self.FILLER: RED,
            self.STUTTER: BLUE,
            self.GAP: DIM,
            self.SPEECH: GREEN,
        }[self]

@dataclass
class Region:
    start: float
    end: float
    kind: RegionKind

@dataclass
class Segment:
    start: float
    end: float
    left: Region | None = field(default=None, repr=False)
    right: Region | None = field(default=None, repr=False)

@dataclass
class VideoInfo:
    codec: str | None = None
    bitrate: int | None = None
    width: int | None = None
    height: int | None = None
    fps: float | None = None
    pix_fmt: str | None = None
    color_space: str | None = None
    color_transfer: str | None = None
    color_primaries: str | None = None
    color_range: str | None = None

    def __str__(self):
        parts = []
        if self.codec:
            parts.append(self.codec)
        if self.width and self.height:
            parts.append(f"{self.width}x{self.height}")
        if self.fps:
            parts.append(f"{self.fps:g}fps")
        if self.pix_fmt:
            parts.append(self.pix_fmt)
        if self.color_space and self.color_space != "unknown":
            parts.append(self.color_space)
        if self.color_range and self.color_range != "unknown":
            parts.append(self.color_range)
        if self.bitrate:
            parts.append(f"{self.bitrate // 1000}k")

        return " ".join(parts) or "?"

@dataclass
class AudioInfo:
    codec: str | None = None
    bitrate: int | None = None
    sample_rate: int | None = None
    channels: int | None = None

    def __str__(self):
        parts = []
        if self.codec:
            parts.append(self.codec)
        if self.sample_rate:
            parts.append(f"{self.sample_rate // 1000}kHz")
        if self.channels:
            parts.append(f"{self.channels}ch")
        if self.bitrate:
            parts.append(f"{self.bitrate // 1000}k")

        return " ".join(parts) or "?"

@dataclass
class MediaInfo:
    video: VideoInfo = field(default_factory=VideoInfo)
    audio: AudioInfo = field(default_factory=AudioInfo)
    size: int | None = None

    @property
    def size_str(self):
        if self.size is None:
            return "?"
        if self.size >= 1_073_741_824:
            return f"{self.size / 1_073_741_824:.2f} GB"
        if self.size >= 1_048_576:
            return f"{self.size / 1_048_576:.1f} MB"
        if self.size >= 1024:
            return f"{self.size / 1024:.1f} KB"

        return f"{self.size} B"

_use_color = sys.stderr.isatty()

def _ansi(code):
    if _use_color:
        return f"\033[{code}m"

    return ""

RESET = _ansi(0)
BOLD = _ansi(1)
DIM = _ansi(2)
GREEN = _ansi(32)
YELLOW = _ansi(33)
RED = _ansi(31)
BLUE = _ansi(34)

def _term_width():
    try:
        return os.get_terminal_size().columns
    except OSError:
        return 80

def section(label):
    w = _term_width()
    pad = max(0, w - len(label) - 2)
    left = pad // 2
    right = pad - left
    line = f"{DIM}{'─' * left}{RESET}{BOLD}{YELLOW} {label} {RESET}{DIM}{'─' * right}{RESET}"
    print(f"\n{line}", file=sys.stderr)

def status(msg):
    print(msg, file=sys.stderr)

def error(msg):
    print(f"{RED}error:{RESET} {msg}", file=sys.stderr)

def format_timestamp(seconds):
    s = float(seconds)
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)

    return f"{int(h):02d}:{int(m):02d}:{s:06.3f}"

FILLER_PATTERN = re.compile(
    r"^("
    r"u+[hm]+|"  # uh, uhh, um, umm, uhm
    r"[hm]+m*|"  # hm, hmm, mm, mmm, mhm
    r"e+r+m*|"  # er, erm, errr
    r"a+h*|"  # ah, ahh, aaa, aaah
    r"o+h+|"  # oh, ohh
    r"e+h+|"  # eh, ehh
    r")$"
)

class Analyzer:
    def __init__(self, noise, duration, model_path=None):
        self.noise = noise
        self.duration = duration
        self.model_path = model_path

    def get_duration(self, input_file):
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-show_entries",
                "format=duration",
                "-of",
                "default=noprint_wrappers=1:nokey=1",
                str(input_file),
            ],
            capture_output=True,
            text=True,
        )
        try:
            return float(result.stdout.strip())
        except ValueError:
            return None

    def detect_silence(self, input_file, total):
        cmd = [
            "ffmpeg",
            "-i",
            str(input_file),
            "-hide_banner",
            "-af",
            f"silencedetect=n={self.noise}:d={self.duration}",
            "-f",
            "null",
            "-",
        ]
        log.debug("silence detect cmd: %s", " ".join(cmd))
        proc = subprocess.Popen(
            cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True
        )
        lines = []
        for line in proc.stderr:
            sys.stderr.write(line)
            lines.append(line)
        proc.wait()
        if proc.returncode != 0:
            raise RuntimeError(f"silence detection failed (exit {proc.returncode})")
        output = "".join(lines)

        silences = []
        silence_start = None
        for line in output.splitlines():
            start_match = re.search(r"silence_start: (\d+\.?\d+)", line)
            end_match = re.search(r"silence_end: (\d+\.?\d+)", line)
            if start_match:
                silence_start = float(start_match.group(1))
            if end_match:
                if silence_start is not None:
                    silences.append(
                        Region(
                            silence_start, float(end_match.group(1)), RegionKind.SILENCE
                        )
                    )
                    silence_start = None
        if silence_start is not None:
            silences.append(Region(silence_start, total, RegionKind.SILENCE))

        return silences

    def _transcribe(self, input_file):
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
            extract = subprocess.run(
                [
                    "ffmpeg",
                    "-i",
                    str(input_file),
                    "-ar",
                    "16000",
                    "-ac",
                    "1",
                    "-f",
                    "wav",
                    "-y",
                    tmp.name,
                ],
                capture_output=True,
            )
            if extract.returncode != 0:
                raise RuntimeError("failed to extract audio for transcription")
            log.info("transcribing %s", input_file)
            result = subprocess.run(
                [
                    "whisper-cli",
                    "-m",
                    str(self.model_path),
                    "-pp",
                    "--max-len",
                    "1",
                    "--split-on-word",
                    "-oj",
                    "-of",
                    tmp.name,
                    tmp.name,
                ],
                stdout=subprocess.DEVNULL,
            )
            if result.returncode != 0:
                raise RuntimeError(f"whisper-cli failed (exit {result.returncode})")

            json_path = f"{tmp.name}.json"
            try:
                with open(json_path) as f:
                    return json.load(f)
            finally:
                Path(json_path).unlink(missing_ok=True)

    @staticmethod
    def _classify_words(transcription):
        speech = []
        fillers = []
        stutters = []
        prev_letters = None
        for seg in transcription.get("transcription", []):
            text = seg["text"].strip()
            letters = re.sub(r"[^a-z]", "", text.lower())
            start = seg["offsets"]["from"] / 1000
            end = seg["offsets"]["to"] / 1000
            if start >= end:
                continue
            is_filler = (not text) or (letters and FILLER_PATTERN.match(letters))
            if is_filler:
                fillers.append(Region(start, end, RegionKind.FILLER))
                log.debug(
                    "  transcribed filler: '%s' at %s -> %s",
                    text,
                    format_timestamp(start),
                    format_timestamp(end),
                )
                prev_letters = None
            elif letters and letters == prev_letters:
                stutters.append(Region(start, end, RegionKind.STUTTER))
                log.debug(
                    "  stutter: '%s' at %s -> %s",
                    text,
                    format_timestamp(start),
                    format_timestamp(end),
                )
            else:
                speech.append(Region(start, end, RegionKind.SPEECH))
                prev_letters = letters if letters else None

        return speech, fillers, stutters

    @staticmethod
    def _find_uncovered_gaps(known_regions):
        known_regions.sort(key=lambda r: r.start)
        merged = []
        for r in known_regions:
            if merged and r.start <= merged[-1].end:
                merged[-1] = Region(
                    merged[-1].start, max(merged[-1].end, r.end), merged[-1].kind
                )
            else:
                merged.append(r)

        gaps = []
        pos = 0.0
        for r in merged:
            if r.start > pos:
                gaps.append(Region(pos, r.start, RegionKind.GAP))
            pos = r.end

        return gaps

    def detect_filler_words(self, input_file, silences):
        data = self._transcribe(input_file)
        if data is None:
            return []

        speech, fillers, stutters = self._classify_words(data)
        gaps = self._find_uncovered_gaps(silences + speech + fillers + stutters)
        regions = fillers + gaps + stutters

        for i, r in enumerate(regions, 1):
            log.info(
                "  %d. %s -> %s (%.1fs) %s",
                i,
                format_timestamp(r.start),
                format_timestamp(r.end),
                r.end - r.start,
                r.kind,
            )
        log.info(
            "speech: %d, fillers: %d, stutters: %d, gaps: %d",
            len(speech),
            len(fillers),
            len(stutters),
            len(gaps),
        )

        return regions

    @staticmethod
    def merge_regions(silences, fillers):
        regions = list(silences) + list(fillers)
        regions.sort(key=lambda r: r.start)

        if not regions:
            return []

        merged = [regions[0]]
        for r in regions[1:]:
            prev = merged[-1]
            if r.start <= prev.end:
                stronger = (
                    prev.kind if prev.kind.priority >= r.kind.priority else r.kind
                )
                merged[-1] = Region(prev.start, max(prev.end, r.end), stronger)
            else:
                merged.append(r)

        return merged

    @staticmethod
    def regions_to_segments(regions, total):
        segments = []
        pos = 0.0
        for region in regions:
            if region.start > pos:
                left = segments[-1].right if segments else None
                segments.append(
                    Segment(start=pos, end=region.start, left=left, right=region)
                )
            pos = region.end
        if pos < total:
            left = segments[-1].right if segments else None
            segments.append(Segment(start=pos, end=total, left=left))

        return segments

class Encoder:
    GPU_ENCODERS = {
        "h264": {"nvidia": "h264_nvenc", "amd": "h264_amf", "vaapi": "h264_vaapi"},
        "hevc": {"nvidia": "hevc_nvenc", "amd": "hevc_amf", "vaapi": "hevc_vaapi"},
        "av1": {"nvidia": "av1_nvenc", "amd": "av1_amf", "vaapi": "av1_vaapi"},
    }

    def __init__(self, gpu, codec, force=False):
        self.gpu = gpu
        self.codec = codec
        self.force = force

    @staticmethod
    def detect_gpu():
        encoders = subprocess.run(
            ["ffmpeg", "-hide_banner", "-encoders"],
            capture_output=True,
            text=True,
        ).stdout

        if "hevc_nvenc" in encoders:
            return "nvidia"
        if "hevc_amf" in encoders:
            return "amd"
        if "hevc_vaapi" in encoders:
            return "vaapi"

        return None

    @staticmethod
    def _ffprobe_stream(input_file, stream_type, fields):
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                f"{stream_type}:0",
                "-show_entries",
                f"stream={','.join(fields)}",
                "-of",
                "json",
                str(input_file),
            ],
            capture_output=True,
            text=True,
        )
        try:
            streams = json.loads(result.stdout).get("streams", [])
            if streams:
                return streams[0]
        except (json.JSONDecodeError, IndexError):
            pass

        return {}

    @staticmethod
    def _parse_fps(r_frame_rate):
        try:
            num, den = r_frame_rate.split("/")

            return round(int(num) / int(den), 3)
        except (ValueError, ZeroDivisionError, AttributeError):
            return None

    @staticmethod
    def probe(input_file):
        vs = Encoder._ffprobe_stream(
            input_file,
            "v",
            [
                "codec_name",
                "bit_rate",
                "width",
                "height",
                "r_frame_rate",
                "pix_fmt",
                "color_space",
                "color_transfer",
                "color_primaries",
                "color_range",
            ],
        )
        a = Encoder._ffprobe_stream(
            input_file,
            "a",
            ["codec_name", "bit_rate", "sample_rate", "channels"],
        )

        def _int(v):
            try:
                return int(v) if v else None
            except (ValueError, TypeError):
                return None

        try:
            size = Path(input_file).stat().st_size
        except OSError:
            size = None

        return MediaInfo(
            size=size,
            video=VideoInfo(
                codec=vs.get("codec_name"),
                bitrate=_int(vs.get("bit_rate")),
                width=_int(vs.get("width")),
                height=_int(vs.get("height")),
                fps=Encoder._parse_fps(vs.get("r_frame_rate")),
                pix_fmt=vs.get("pix_fmt"),
                color_space=vs.get("color_space"),
                color_transfer=vs.get("color_transfer"),
                color_primaries=vs.get("color_primaries"),
                color_range=vs.get("color_range"),
            ),
            audio=AudioInfo(
                codec=a.get("codec_name"),
                bitrate=_int(a.get("bit_rate")),
                sample_rate=_int(a.get("sample_rate")),
                channels=_int(a.get("channels")),
            ),
        )

    def _video_codec_args(self, info):
        if self.codec:
            return ["-c:v", self.codec]

        family = None
        if info.codec:
            name = info.codec.lower()
            if name in ("h264", "libx264"):
                family = "h264"
            elif name in ("hevc", "h265", "libx265"):
                family = "hevc"
            elif name in ("av1", "libsvtav1", "libaom-av1"):
                family = "av1"

        args = []
        if self.gpu and family and family in self.GPU_ENCODERS:
            hw_enc = self.GPU_ENCODERS[family].get(self.gpu)
            if hw_enc:
                if self.gpu == "vaapi":
                    args = [
                        "-vaapi_device",
                        "/dev/dri/renderD128",
                        "-c:v",
                        hw_enc,
                        "-qp",
                        "20",
                    ]
                elif self.gpu == "nvidia":
                    args = [
                        "-c:v",
                        hw_enc,
                        "-preset",
                        "p5",
                        "-tune",
                        "hq",
                        "-rc",
                        "constqp",
                        "-qp",
                        "20",
                        "-multipass",
                        "qres",
                        "-bf",
                        "2",
                    ]
                else:
                    args = ["-c:v", hw_enc, "-qp", "20"]
        elif info.codec:
            args = ["-c:v", info.codec, "-crf", "20"]

        if not args:
            return []
        if info.pix_fmt:
            args.extend(["-pix_fmt", info.pix_fmt])
        if info.color_space and info.color_space != "unknown":
            args.extend(["-colorspace", info.color_space])
        if info.color_transfer and info.color_transfer != "unknown":
            args.extend(["-color_trc", info.color_transfer])
        if info.color_primaries and info.color_primaries != "unknown":
            args.extend(["-color_primaries", info.color_primaries])
        if info.color_range and info.color_range != "unknown":
            cr = (
                "mpeg"
                if info.color_range == "tv"
                else "jpeg"
                if info.color_range == "pc"
                else info.color_range
            )
            args.extend(["-color_range", cr])

        return args

    def _audio_codec_args(self, info):
        args = ["-c:a", info.codec or "aac"]
        if info.bitrate:
            args.extend(["-b:a", str(info.bitrate)])

        return args

    @staticmethod
    def _run(cmd):
        log.debug("encode cmd: %s", " ".join(cmd))

        return subprocess.run(cmd)

    def encode(self, input_file, output_file, segments, media=None):
        media = media or MediaInfo()

        if len(segments) == 1:
            seg = segments[0]
            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-ss",
                str(seg.start),
                "-to",
                str(seg.end),
                "-i",
                str(input_file),
                "-c:v",
                "copy",
                "-c:a",
                "copy",
            ]
            if self.force:
                cmd.append("-y")
            cmd.append(str(output_file))

            return self._run(cmd)

        return self._encode_segments(input_file, output_file, segments, media)

    def _encode_segments(self, input_file, output_file, segments, media):
        raise NotImplementedError

class FancyEncoder(Encoder):
    def __init__(self, gpu, codec, fade_time, video_filter, audio_filter, force=False):
        super().__init__(gpu, codec, force)
        self.fade_time = fade_time
        self.video_filter = video_filter
        self.audio_filter = audio_filter

    def _xfade_expr(self, offset):
        name, _, extra = self.video_filter.partition(":")
        parts = [f"{name}=offset={offset}:duration={self.fade_time}"]
        if extra:
            parts.append(extra)

        return ":".join(parts)

    def _acrossfade_expr(self):
        name, _, extra = self.audio_filter.partition(":")
        parts = [f"{name}=duration={self.fade_time}"]
        if extra:
            parts.append(extra)

        return ":".join(parts)

    @staticmethod
    @contextmanager
    def _filter_script(lines):
        content = ";\n".join(lines)
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".txt", prefix="remsi_filter_", delete=True
        ) as f:
            f.write(content)
            f.flush()
            log.debug("filter_complex_script %s:\n%s", f.name, content)

            yield f.name

    def _should_crossfade(self, i, segments):
        return (
            self.fade_time > 0
            and i < len(segments) - 1
            and segments[i].right is not None
            and segments[i].right.kind == RegionKind.SILENCE
            and (segments[i].end - segments[i].start) >= self.fade_time
            and (segments[i + 1].end - segments[i + 1].start) >= self.fade_time
        )

    def _build_filter_lines(self, segments, video_info=None):
        n = len(segments)

        color_filters = ""
        if video_info:
            parts = []
            if video_info.pix_fmt:
                parts.append(f"format={video_info.pix_fmt}")
            sp = []
            if video_info.color_space and video_info.color_space != "unknown":
                sp.append(f"colorspace={video_info.color_space}")
            if video_info.color_transfer and video_info.color_transfer != "unknown":
                sp.append(f"color_trc={video_info.color_transfer}")
            if video_info.color_primaries and video_info.color_primaries != "unknown":
                sp.append(f"color_primaries={video_info.color_primaries}")
            if video_info.color_range and video_info.color_range != "unknown":
                r = (
                    "limited"
                    if video_info.color_range == "tv"
                    else "full"
                    if video_info.color_range == "pc"
                    else video_info.color_range
                )
                sp.append(f"range={r}")
            if sp:
                parts.append("setparams=" + ":".join(sp))
            if parts:
                color_filters = "," + ",".join(parts)

        lines = []
        for i, seg in enumerate(segments):
            lines.append(
                f"[0:v]trim={seg.start}:{seg.end},setpts=PTS-STARTPTS,settb=AVTB{color_filters}[v{i}]"
            )
            lines.append(f"[0:a]atrim={seg.start}:{seg.end},asetpts=PTS-STARTPTS[a{i}]")

        if self.fade_time > 0 and n > 1:
            accumulated = 0.0
            xfade_count = 0
            v_label = "[v0]"
            for i in range(n - 1):
                accumulated += segments[i].end - segments[i].start
                if self._should_crossfade(i, segments):
                    offset = accumulated - (xfade_count + 1) * self.fade_time
                    out = f"[xf{xfade_count}]"
                    lines.append(f"{v_label}[v{i + 1}]{self._xfade_expr(offset)}{out}")
                    v_label = out
                    xfade_count += 1
                else:
                    out = f"[vc{i}]"
                    lines.append(f"{v_label}[v{i + 1}]concat=n=2:v=1:a=0{out}")
                    v_label = out
            lines.append(f"{v_label}null[vout]")
        else:
            vf_labels = "".join(f"[v{i}]" for i in range(n))
            lines.append(f"{vf_labels}concat=n={n}:v=1:a=0[vout]")

        if self.fade_time > 0 and n > 1:
            a_label = "[a0]"
            for i in range(n - 1):
                if self._should_crossfade(i, segments):
                    out = f"[ax{i}]"
                    lines.append(f"{a_label}[a{i + 1}]{self._acrossfade_expr()}{out}")
                    a_label = out
                else:
                    out = f"[ac{i}]"
                    lines.append(f"{a_label}[a{i + 1}]concat=n=2:v=0:a=1{out}")
                    a_label = out
            lines.append(f"{a_label}anull[aout]")
        else:
            af_labels = "".join(f"[a{i}]" for i in range(n))
            lines.append(f"{af_labels}concat=n={n}:v=0:a=1[aout]")

        return lines

    def _encode_segments(self, input_file, output_file, segments, media):
        lines = self._build_filter_lines(segments, media.video)
        with self._filter_script(lines) as script_path:
            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-i",
                str(input_file),
                "-/filter_complex",
                script_path,
                "-map",
                "[vout]",
                "-map",
                "[aout]",
            ]
            if self.force:
                cmd.append("-y")
            cmd.extend(self._video_codec_args(media.video))
            cmd.extend(self._audio_codec_args(media.audio))
            cmd.append(str(output_file))

            return self._run(cmd)

class SmartCutEncoder(Encoder):
    @staticmethod
    def _probe_keyframes(input_file):
        result = subprocess.run(
            [
                "ffprobe",
                "-v",
                "error",
                "-select_streams",
                "v:0",
                "-skip_frame",
                "nokey",
                "-show_entries",
                "frame=pts_time",
                "-of",
                "csv=p=0",
                str(input_file),
            ],
            capture_output=True,
            text=True,
        )
        keyframes = []
        for line in result.stdout.splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                keyframes.append(float(line))
            except ValueError:
                continue
        keyframes.sort()
        log.info("probed %d keyframes", len(keyframes))

        return keyframes

    @staticmethod
    def _split_at_keyframes(seg_start, seg_end, keyframes):
        if not keyframes:
            return [(seg_start, seg_end, True)]

        idx_start = bisect.bisect_left(keyframes, seg_start)
        kf_start = keyframes[idx_start] if idx_start < len(keyframes) else None

        idx_end = bisect.bisect_right(keyframes, seg_end) - 1
        kf_end = keyframes[idx_end] if idx_end >= 0 else None

        if kf_start is None or kf_end is None or kf_start >= seg_end:
            return [(seg_start, seg_end, True)]

        parts = []
        if kf_start > seg_start:
            parts.append((seg_start, kf_start, True))
        if kf_end > kf_start:
            parts.append((kf_start, kf_end, False))
        elif kf_start < seg_end:
            parts.append((kf_start, seg_end, True))

            return parts
        if seg_end > kf_end:
            parts.append((kf_end, seg_end, True))

        return parts if parts else [(seg_start, seg_end, True)]

    def __init__(self, gpu, codec, fade_time, force=False):
        super().__init__(gpu, codec, force)
        self.fade_time = fade_time

    @staticmethod
    def _encode_part(
        input_file,
        part_path,
        start,
        end,
        video_args,
        audio_args,
        fade_time=0,
        fade_in=False,
        fade_out=False,
    ):
        duration = end - start
        cmd = [
            "ffmpeg",
            "-hide_banner",
            "-loglevel",
            "warning",
            "-ss",
            str(start),
            "-i",
            str(input_file),
            "-t",
            str(duration),
        ]
        cmd.extend(video_args)
        af_parts = []
        if fade_in and fade_time > 0:
            af_parts.append(f"afade=t=in:d={fade_time}")
        if fade_out and fade_time > 0:
            af_parts.append(f"afade=t=out:st={duration - fade_time}:d={fade_time}")
        if af_parts:
            cmd.extend(["-af", ",".join(af_parts)])
        cmd.extend(audio_args)
        cmd.extend(["-y", str(part_path)])
        log.debug("fast part [reencode]: %s", " ".join(cmd))

        return subprocess.run(cmd)

    def _encode_segments(self, input_file, output_file, segments, media):  # noqa: C901
        keyframes = self._probe_keyframes(input_file)
        video_args = self._video_codec_args(media.video)
        audio_args = self._audio_codec_args(media.audio)

        all_parts = []
        for seg in segments:
            sub = self._split_at_keyframes(seg.start, seg.end, keyframes)
            for start, end, reencode in sub:
                log.info(
                    "  part %s -> %s (%s)",
                    format_timestamp(start),
                    format_timestamp(end),
                    "reencode" if reencode else "copy",
                )
            all_parts.extend(sub)

        copy_dur = sum(e - s for s, e, r in all_parts if not r)
        reencode_dur = sum(e - s for s, e, r in all_parts if r)
        status(
            f"smart-cut:\t{GREEN}{copy_dur:.1f}s{RESET} stream copy, "
            f"{YELLOW}{reencode_dur:.1f}s{RESET} re-encode"
        )

        src = str(input_file.resolve())
        with tempfile.TemporaryDirectory(prefix="remsi_fast_") as tmpdir:
            tmpdir = Path(tmpdir)
            concat_entries = []
            reencode_idx = 0

            n = len(all_parts)
            for i, (start, end, reencode) in enumerate(all_parts, 1):
                mode = "reencode" if reencode else "copy"
                status(
                    f"  [{i}/{n}] {format_timestamp(start)} -> "
                    f"{format_timestamp(end)} ({mode})"
                )
                if reencode:
                    part_path = tmpdir / f"part{reencode_idx:04d}{input_file.suffix}"
                    reencode_idx += 1
                    idx = i - 1
                    fade_in = idx > 0 and not all_parts[idx - 1][2]
                    fade_out = idx < n - 1 and not all_parts[idx + 1][2]
                    result = self._encode_part(
                        input_file,
                        part_path,
                        start,
                        end,
                        video_args,
                        audio_args,
                        fade_time=self.fade_time,
                        fade_in=fade_in,
                        fade_out=fade_out,
                    )
                    if result.returncode != 0:
                        return result
                    concat_entries.append(f"file '{part_path}'\n")
                else:
                    concat_entries.append(
                        f"file '{src}'\ninpoint {start}\noutpoint {end}\n"
                    )

            section("Concat")
            concat_list = tmpdir / "concat.txt"
            with open(concat_list, "w") as f:
                f.writelines(concat_entries)

            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-stats",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(concat_list),
                "-c:v",
                "copy",
            ]
            cmd.extend(audio_args)
            if self.force:
                cmd.append("-y")
            cmd.append(str(output_file))

            return self._run(cmd)

class Remsi:
    def __init__(self, args):
        self.args = args
        self.analyzer = Analyzer(
            noise=args.noise,
            duration=args.duration,
            model_path=self._resolve_model_path(),
        )
        if args.mode == "fast":
            self.encoder = SmartCutEncoder(
                gpu=self._resolve_gpu(),
                codec=args.codec,
                fade_time=args.fade_time,
                force=args.force,
            )
        else:
            self.encoder = FancyEncoder(
                gpu=self._resolve_gpu(),
                codec=args.codec,
                fade_time=args.fade_time,
                video_filter=args.fade_video_filter,
                audio_filter=args.fade_audio_filter,
                force=args.force,
            )

    def _resolve_gpu(self):
        if self.args.gpu == "auto":
            gpu = Encoder.detect_gpu()
            if gpu:
                log.info("detected gpu encoder: %s", gpu)
                status(f"GPU encoder: {GREEN}{gpu}{RESET}")
            else:
                log.warning("no gpu encoder found, using software encoding")

            return gpu
        if self.args.gpu != "none":
            return self.args.gpu

        return None

    def _resolve_model_path(self):
        if not self.args.whisper:
            return None

        model_path = Path(self.args.model_dir).expanduser() / self.args.model
        if not model_path.exists():
            error(f"model not found at {model_path}")
            sys.exit(1)

        return model_path

    def _process_file(self, input_file, output_file):
        section(f"Processing {input_file.name}")
        t_start = time.monotonic()

        total = self.analyzer.get_duration(input_file)
        if total is None:
            error(f"could not determine duration of {input_file}")

            return
        media = Encoder.probe(input_file)
        status(f"duration:\t{BOLD}{format_timestamp(total)}{RESET}")
        status(f"size:\t\t{YELLOW}{media.size_str}{RESET}")
        status(f"video:\t\t{YELLOW}{media.video}{RESET}")
        status(f"audio:\t\t{YELLOW}{media.audio}{RESET}")
        log.debug(
            "silence params: noise=%s duration=%s", self.args.noise, self.args.duration
        )

        t_probe = time.monotonic()
        try:
            section("Silence Detection")
            silences = self.analyzer.detect_silence(input_file, total)
            status(f"found {YELLOW}{len(silences)}{RESET} silent region(s)")

            fillers = []
            if self.analyzer.model_path:
                section("Speech Analysis")
                fillers = self.analyzer.detect_filler_words(input_file, silences)
                n_filler = sum(1 for r in fillers if r.kind == RegionKind.FILLER)
                n_stutter = sum(1 for r in fillers if r.kind == RegionKind.STUTTER)
                parts = []
                if n_filler:
                    parts.append(f"{YELLOW}{n_filler}{RESET} filler(s)")
                if n_stutter:
                    parts.append(f"{YELLOW}{n_stutter}{RESET} stutter(s)")
                if parts:
                    status(f"found {', '.join(parts)}")
        except RuntimeError as e:
            error(str(e))

            return

        all_regions = Analyzer.merge_regions(silences, fillers)
        min_cut = self.args.min_cut
        if min_cut > 0:
            skipped = [r for r in all_regions if (r.end - r.start) < min_cut]
            cut = [r for r in all_regions if (r.end - r.start) >= min_cut]
            if skipped:
                status(
                    f"skipping {YELLOW}{len(skipped)}{RESET} region(s) "
                    f"shorter than {min_cut}s"
                )
            all_regions = cut
        segments = self.analyzer.regions_to_segments(all_regions, total)

        if not all_regions:
            status(f"{DIM}nothing to remove, skipping{RESET}")

            return

        kept = sum(seg.end - seg.start for seg in segments)
        removed = total - kept
        pct = (removed / total * 100) if total > 0 else 0
        status(
            f"removing {YELLOW}{removed:.1f}s{RESET} of {total:.1f}s "
            f"({BOLD}{pct:.1f}%{RESET})"
        )

        section("Summary")
        timeline = [
            Region(seg.start, seg.end, RegionKind.SPEECH) for seg in segments
        ] + list(all_regions)
        timeline.sort(key=lambda r: r.start)
        for i, r in enumerate(timeline, 1):
            status(
                f"{DIM}{i:3d}.{RESET} {format_timestamp(r.start)} -> "
                f"{format_timestamp(r.end)} {DIM}({r.end - r.start:.1f}s){RESET} "
                f"{r.kind.color()}{r.kind}{RESET}"
            )

        t_analysis = time.monotonic()

        if self.args.analyze:
            t_total = t_analysis - t_start
            status(
                f"elapsed:\t{YELLOW}{t_total:.1f}s{RESET} "
                f"({DIM}probe {t_probe - t_start:.1f}s, "
                f"analysis {t_analysis - t_probe:.1f}s{RESET})"
            )

            return

        section("Encoding")
        if isinstance(self.encoder, SmartCutEncoder):
            status(f"mode:\t\t{YELLOW}fast (smart-cut){RESET}")
        status(f"writing {GREEN}{output_file}{RESET}")
        try:
            result = self.encoder.encode(input_file, output_file, segments, media)
        except KeyboardInterrupt:
            error("interrupted, cleaning up...")
            sys.exit(130)

        if result.returncode != 0:
            error(f"ffmpeg exited with code {result.returncode}")

            return

        section(f"{input_file.name} -> {output_file.stem}")
        parts = []
        if silences:
            parts.append(f"{YELLOW}{len(silences)}{RESET} silence(s)")
        n_filler = sum(1 for r in fillers if r.kind == RegionKind.FILLER)
        n_stutter = sum(1 for r in fillers if r.kind == RegionKind.STUTTER)
        if n_filler:
            parts.append(f"{YELLOW}{n_filler}{RESET} filler(s)")
        if n_stutter:
            parts.append(f"{YELLOW}{n_stutter}{RESET} stutter(s)")
        if parts:
            status(f"detected {' and '.join(parts)}")
        status(
            f"trimmed {YELLOW}{removed:.1f}s{RESET} ({BOLD}{pct:.1f}%{RESET}), "
            f"{format_timestamp(total)} down to {GREEN}{format_timestamp(kept)}{RESET}"
        )
        out_media = Encoder.probe(output_file)
        status(f"size:\t\t{YELLOW}{out_media.size_str}{RESET}")
        status(f"video:\t\t{YELLOW}{out_media.video}{RESET}")
        status(f"audio:\t\t{YELLOW}{out_media.audio}{RESET}")
        t_end = time.monotonic()
        t_total = t_end - t_start
        status(
            f"elapsed:\t{YELLOW}{t_total:.1f}s{RESET} "
            f"({DIM}probe {t_probe - t_start:.1f}s, "
            f"analysis {t_analysis - t_probe:.1f}s, "
            f"encode {t_end - t_analysis:.1f}s{RESET})"
        )

        status(f"\n{GREEN}{output_file}{RESET}")

    def run(self):
        for input_path in self.args.input:
            input_file = Path(input_path)
            if not input_file.exists():
                error(f"{input_file} not found")
                continue

            if self.args.output:
                output_file = Path(self.args.output)
            else:
                output_file = input_file.with_stem(
                    f"{input_file.stem}-{self.args.suffix}"
                )

            self._process_file(input_file, output_file)

def main():
    parser = argparse.ArgumentParser(
        description="Remove silent parts from video files using FFmpeg.",
    )
    parser.add_argument("input", nargs="+", help="input video file(s)")
    parser.add_argument("-o", "--output", help="output file, only with single input")
    parser.add_argument("-n", "--noise", default="-45dB", help="silence threshold")
    parser.add_argument(
        "-d",
        "--duration",
        type=float,
        default=0.8,
        help="minimum silence duration in seconds",
    )
    parser.add_argument(
        "--min-cut",
        type=float,
        default=0.05,
        help="minimum region duration to cut (smaller regions are kept)",
    )
    parser.add_argument(
        "--gpu",
        choices=["nvidia", "amd", "vaapi", "auto", "none"],
        default="auto",
        help="GPU acceleration",
    )
    parser.add_argument("--codec", help="override video codec")
    parser.add_argument(
        "--fade-time",
        type=float,
        default=0.1,
        help="crossfade duration in seconds (0 to disable)",
    )
    parser.add_argument(
        "--fade-video-filter",
        default="xfade:transition=fadefast",
        help="video crossfade filter spec (name[:extra_params])",
    )
    parser.add_argument(
        "--fade-audio-filter",
        default="acrossfade:curve1=log:curve2=log",
        help="audio crossfade filter spec (name[:extra_params])",
    )
    parser.add_argument("--suffix", default="silencer", help="output filename suffix")
    parser.add_argument(
        "--whisper",
        action="store_true",
        help="also detect filler words using speech recognition",
    )
    parser.add_argument(
        "--model-dir",
        default="~/.local/share/applications/waystt/models",
        help="directory containing whisper GGML models",
    )
    parser.add_argument(
        "--model",
        default="ggml-large-v3.bin",
        help="whisper GGML model filename",
    )
    parser.add_argument(
        "--mode",
        choices=["fancy", "fast"],
        default="fancy",
        help="encoding mode: fancy (crossfade/filter_complex) or fast (smart-cut)",
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="analyze only: detect regions and show summary without encoding",
    )
    parser.add_argument(
        "-f", "--force", action="store_true", help="overwrite output file if it exists"
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="verbose logging")
    args = parser.parse_args()

    logging.basicConfig(
        format="%(name)s: %(message)s",
        level=logging.DEBUG if args.verbose else logging.WARNING,
    )

    if args.output and len(args.input) > 1:
        parser.error("-o/--output can only be used with a single input file")

    Remsi(args).run()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nInterrupted", file=sys.stderr)
        sys.exit(130)
