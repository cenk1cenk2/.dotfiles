#!/usr/bin/env python3

import argparse
import json
import logging
import re
import subprocess
import sys
import tempfile
from collections import namedtuple
from pathlib import Path

log = logging.getLogger("remsi")

Segment = namedtuple("Segment", ["start", "end", "fade_in", "fade_out"])

_use_color = sys.stderr.isatty()

def _ansi(code):
    if _use_color:
        return f"\033[{code}m"

    return ""

RESET = _ansi(0)
BOLD = _ansi(1)
DIM = _ansi(2)
CYAN = _ansi(36)
GREEN = _ansi(32)
YELLOW = _ansi(33)
RED = _ansi(31)
MAGENTA = _ansi(35)

def section(label):
    line = f"{DIM}{'â”€' * 48}{RESET}"
    print(f"\n{line}\n{BOLD}{CYAN}{label}{RESET}\n{line}", file=sys.stderr)

def status(msg):
    print(msg, file=sys.stderr)

def error(msg):
    print(f"{RED}error:{RESET} {msg}", file=sys.stderr)

def success(msg):
    print(f"{GREEN}{msg}{RESET}", file=sys.stderr)

def detect_gpu():
    encoders = subprocess.run(
        ["ffmpeg", "-hide_banner", "-encoders"],
        capture_output=True,
        text=True,
    ).stdout

    if "hevc_nvenc" in encoders:
        return "nvidia"
    if "hevc_amf" in encoders:
        return "amd"
    if "hevc_vaapi" in encoders:
        return "vaapi"

    return None

def gpu_encode_args(gpu):
    if gpu == "nvidia":
        return ["-c:v", "hevc_nvenc", "-preset", "p4", "-cq", "23"]
    if gpu == "amd":
        return [
            "-c:v",
            "hevc_amf",
            "-quality",
            "balanced",
            "-rc",
            "vbr_latency",
            "-qp_i",
            "23",
            "-qp_p",
            "23",
        ]
    if gpu == "vaapi":
        return [
            "-vaapi_device",
            "/dev/dri/renderD128",
            "-c:v",
            "hevc_vaapi",
            "-qp",
            "23",
        ]

    return []

def format_timestamp(seconds):
    s = float(seconds)
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)

    return f"{int(h):02d}:{int(m):02d}:{s:06.3f}"

def get_duration(input_file):
    result = subprocess.run(
        [
            "ffprobe",
            "-v",
            "error",
            "-show_entries",
            "format=duration",
            "-of",
            "default=noprint_wrappers=1:nokey=1",
            str(input_file),
        ],
        capture_output=True,
        text=True,
    )
    try:
        return float(result.stdout.strip())
    except ValueError:
        return None

def detect_silence(input_file, noise_db, duration, total):
    cmd = [
        "ffmpeg",
        "-i",
        str(input_file),
        "-hide_banner",
        "-af",
        f"silencedetect=n={noise_db}:d={duration}",
        "-f",
        "null",
        "-",
    ]
    log.debug("silence detect cmd: %s", " ".join(cmd))
    proc = subprocess.Popen(
        cmd, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE, text=True
    )
    lines = []
    for line in proc.stderr:
        sys.stderr.write(line)
        lines.append(line)
    proc.wait()
    output = "".join(lines)

    silences = []
    silence_start = None

    for line in output.splitlines():
        start_match = re.search(r"silence_start: (\d+\.?\d+)", line)
        end_match = re.search(r"silence_end: (\d+\.?\d+)", line)

        if start_match:
            silence_start = float(start_match.group(1))
        if end_match:
            if silence_start is not None:
                silences.append((silence_start, float(end_match.group(1))))
                silence_start = None

    if silence_start is not None:
        silences.append((silence_start, total))

    return silences

FILLER_WORDS = {
    "um",
    "umm",
    "uh",
    "uhh",
    "uhm",
    "hmm",
    "hm",
    "mm",
    "mmm",
    "mhm",
    "erm",
    "er",
    "ah",
    "ahh",
}

def transcribe(input_file, model_path):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=True) as tmp:
        extract = subprocess.run(
            [
                "ffmpeg",
                "-i",
                str(input_file),
                "-ar",
                "16000",
                "-ac",
                "1",
                "-f",
                "wav",
                "-y",
                tmp.name,
            ],
            capture_output=True,
        )
        if extract.returncode != 0:
            error("failed to extract audio")

            return None
        log.info("transcribing %s", input_file)
        result = subprocess.run(
            [
                "whisper-cli",
                "-m",
                str(model_path),
                "-pp",
                "--max-len",
                "1",
                "--split-on-word",
                "-oj",
                "-of",
                tmp.name,
                tmp.name,
            ],
            capture_output=True,
        )
        if result.returncode != 0:
            error("whisper-cli failed")

            return None

        json_path = f"{tmp.name}.json"
        try:
            with open(json_path) as f:
                return json.load(f)
        finally:
            Path(json_path).unlink(missing_ok=True)

def classify_words(transcription):
    speech = []
    fillers = []
    for seg in transcription.get("transcription", []):
        text = seg["text"].strip()
        letters = re.sub(r"[^a-z]", "", text.lower())
        start = seg["offsets"]["from"] / 1000
        end = seg["offsets"]["to"] / 1000
        if start >= end:
            continue
        is_filler = (not text) or (letters and letters in FILLER_WORDS)
        if is_filler:
            fillers.append((start, end))
            log.debug(
                "  transcribed filler: '%s' at %s -> %s",
                text,
                format_timestamp(start),
                format_timestamp(end),
            )
        else:
            speech.append((start, end))

    return speech, fillers

def find_uncovered_gaps(known_regions):
    known_regions.sort()
    merged = []
    for start, end in known_regions:
        if merged and start <= merged[-1][1]:
            merged[-1] = (merged[-1][0], max(merged[-1][1], end))
        else:
            merged.append((start, end))

    gaps = []
    pos = 0.0
    for start, end in merged:
        if start > pos:
            gaps.append((pos, start))
        pos = end

    return gaps

def detect_filler_words(input_file, model_path, silences):
    data = transcribe(input_file, model_path)
    if data is None:
        return []

    speech, fillers = classify_words(data)

    gaps = find_uncovered_gaps(silences + speech + fillers)
    fillers.extend(gaps)

    for i, (start, end) in enumerate(fillers, 1):
        log.info(
            "  %d. %s -> %s (%.1fs) filler",
            i,
            format_timestamp(start),
            format_timestamp(end),
            end - start,
        )
    log.info("speech segments: %d, filler gaps: %d", len(speech), len(fillers))

    return fillers

def merge_regions(silences, fillers):
    regions = list(silences) + list(fillers)
    regions.sort()

    if not regions:
        return []

    merged = [regions[0]]
    for start, end in regions[1:]:
        prev_start, prev_end = merged[-1]
        if start <= prev_end:
            merged[-1] = (prev_start, max(prev_end, end))
        else:
            merged.append((start, end))

    return merged

def regions_to_segments(regions, total, silence_dur):
    boundaries = []
    gap_is_silence = []
    pos = 0.0
    for start, end in regions:
        if start > pos:
            boundaries.append((pos, start))
            gap_is_silence.append(end - start >= silence_dur)
        pos = end
    if pos < total:
        boundaries.append((pos, total))

    n = len(boundaries)
    fi = [False] * n
    fo = [False] * n
    for i, is_silence in enumerate(gap_is_silence):
        fo[i] = is_silence
        fi[i + 1] = is_silence

    return [
        Segment(start=s, end=e, fade_in=fade_i, fade_out=fade_o)
        for (s, e), fade_i, fade_o in zip(boundaries, fi, fo)
    ]

def video_codec_args(gpu, codec):
    if gpu and not codec:
        return gpu_encode_args(gpu)
    if codec:
        return ["-c:v", codec]

    return []

def build_filter_command(input_file, output_file, segments, gpu, codec, fade):
    n = len(segments)

    vf_parts = []
    af_parts = []
    for i, seg in enumerate(segments):
        vf_parts.append(f"[0:v]trim={seg.start}:{seg.end},setpts=PTS-STARTPTS[v{i}]")
        af_parts.append(f"[0:a]atrim={seg.start}:{seg.end},asetpts=PTS-STARTPTS[a{i}]")

    filter_lines = vf_parts + af_parts

    has_fades = fade > 0 and n > 1 and any(s.fade_in or s.fade_out for s in segments)
    if has_fades:
        for i, seg in enumerate(segments):
            dur = seg.end - seg.start
            af = f"[a{i}]"
            parts = []
            if seg.fade_in:
                parts.append(f"afade=t=in:d={fade}")
            if seg.fade_out:
                parts.append(f"afade=t=out:st={max(0, dur - fade)}:d={fade}")
            if parts:
                filter_lines.append(f"{af}{','.join(parts)}[a{i}f]")
            else:
                filter_lines.append(f"{af}anull[a{i}f]")
        af_labels = "".join(f"[a{i}f]" for i in range(n))
    else:
        af_labels = "".join(f"[a{i}]" for i in range(n))

    concat_v = "".join(f"[v{i}]" for i in range(n))
    filter_lines.append(f"{concat_v}concat=n={n}:v=1:a=0[vout]")
    filter_lines.append(f"{af_labels}concat=n={n}:v=0:a=1[aout]")

    filter_complex = ";".join(filter_lines)
    log.debug("filter_complex: %s", filter_complex)

    cmd = [
        "ffmpeg",
        "-hide_banner",
        "-loglevel",
        "warning",
        "-stats",
        "-i",
        str(input_file),
        "-filter_complex",
        filter_complex,
        "-map",
        "[vout]",
        "-map",
        "[aout]",
    ]
    cmd.extend(video_codec_args(gpu, codec))
    cmd.extend(["-c:a", "aac", str(output_file)])

    return cmd

def run_concat(input_file, output_file, segments, gpu, codec, fade):
    with tempfile.TemporaryDirectory(prefix="remsi_") as seg_dir:
        seg_paths = []
        for i, seg in enumerate(segments):
            seg_path = Path(seg_dir) / f"seg{i:04d}{input_file.suffix}"
            seg_paths.append(seg_path)
            af_filters = []
            if fade > 0:
                dur = seg.end - seg.start
                if seg.fade_in:
                    af_filters.append(f"afade=t=in:d={fade}")
                if seg.fade_out:
                    af_filters.append(f"afade=t=out:st={max(0, dur - fade)}:d={fade}")
            cmd = [
                "ffmpeg",
                "-hide_banner",
                "-loglevel",
                "warning",
                "-ss",
                str(seg.start),
                "-to",
                str(seg.end),
                "-i",
                str(input_file),
            ]
            if af_filters:
                cmd.extend(["-af", ",".join(af_filters)])
            cmd.extend(video_codec_args(gpu, codec))
            cmd.extend(["-c:a", "aac", "-y", str(seg_path)])
            log.debug("segment %d cmd: %s", i, " ".join(cmd))
            result = subprocess.run(cmd)
            if result.returncode != 0:
                error(f"segment {i} encoding failed")

                return None

        concat_file = Path(seg_dir) / "concat.txt"
        with open(concat_file, "w") as f:
            for p in seg_paths:
                f.write(f"file '{p}'\n")

        cmd = [
            "ffmpeg",
            "-hide_banner",
            "-loglevel",
            "warning",
            "-stats",
            "-f",
            "concat",
            "-safe",
            "0",
            "-i",
            str(concat_file),
            "-c",
            "copy",
            str(output_file),
        ]
        log.debug("concat cmd: %s", " ".join(cmd))

        return subprocess.run(cmd)

CONCAT_THRESHOLD = 100

def build_command(input_file, output_file, segments, gpu, codec, fade):
    n = len(segments)

    if n == 1:
        seg = segments[0]
        cmd = [
            "ffmpeg",
            "-hide_banner",
            "-loglevel",
            "warning",
            "-stats",
            "-ss",
            str(seg.start),
            "-to",
            str(seg.end),
            "-i",
            str(input_file),
        ]
        cmd.extend(video_codec_args(gpu, codec))
        cmd.extend(["-c:a", "aac", str(output_file)])

        return cmd

    if n >= CONCAT_THRESHOLD:
        return run_concat(input_file, output_file, segments, gpu, codec, fade)

    return build_filter_command(input_file, output_file, segments, gpu, codec, fade)

def main():
    parser = argparse.ArgumentParser(
        description="Remove silent parts from video files using FFmpeg.",
    )
    parser.add_argument("input", nargs="+", help="input video file(s)")
    parser.add_argument("-o", "--output", help="output file, only with single input")
    parser.add_argument("-n", "--noise", default="-45dB", help="silence threshold")
    parser.add_argument(
        "-d",
        "--duration",
        type=float,
        default=0.8,
        help="minimum silence duration in seconds",
    )
    parser.add_argument(
        "--gpu",
        choices=["nvidia", "amd", "vaapi", "auto", "none"],
        default="auto",
        help="GPU acceleration",
    )
    parser.add_argument("--codec", help="override video codec")
    parser.add_argument(
        "--fade",
        type=float,
        default=0.1,
        help="crossfade duration in seconds between segments",
    )
    parser.add_argument(
        "--suffix",
        default="silencer",
        help="output filename suffix",
    )
    parser.add_argument(
        "--whisper",
        action="store_true",
        help="also detect filler words using speech recognition",
    )
    parser.add_argument(
        "--model-dir",
        default="~/.local/share/applications/waystt/models",
        help="directory containing whisper GGML models",
    )
    parser.add_argument(
        "--model",
        default="ggml-large-v3.bin",
        help="whisper GGML model filename",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="verbose logging",
    )
    args = parser.parse_args()

    logging.basicConfig(
        format="%(name)s: %(message)s",
        level=logging.DEBUG if args.verbose else logging.WARNING,
    )

    if args.output and len(args.input) > 1:
        parser.error("-o/--output can only be used with a single input file")

    gpu = None
    if args.gpu == "auto":
        gpu = detect_gpu()
        if gpu:
            log.info("detected gpu encoder: %s", gpu)
            status(f"GPU encoder: {MAGENTA}{gpu}{RESET} (HEVC)")
        else:
            log.warning("no gpu encoder found, using software encoding")
    elif args.gpu != "none":
        gpu = args.gpu

    model_path = None
    if args.whisper:
        model_path = Path(args.model_dir).expanduser() / args.model
        if not model_path.exists():
            error(f"model not found at {model_path}")
            sys.exit(1)

    for input_path in args.input:
        input_file = Path(input_path)
        if not input_file.exists():
            error(f"{input_file} not found")
            continue

        if args.output:
            output_file = Path(args.output)
        else:
            output_file = input_file.with_stem(f"{input_file.stem}-{args.suffix}")

        section(f"Processing {input_file.name}")

        total = get_duration(input_file)
        if total is None:
            error(f"could not determine duration of {input_file}")
            continue
        status(f"duration: {BOLD}{format_timestamp(total)}{RESET}")
        log.debug("silence params: noise=%s duration=%s", args.noise, args.duration)

        section("Silence Detection")
        silences = detect_silence(input_file, args.noise, args.duration, total)
        status(f"found {YELLOW}{len(silences)}{RESET} silent region(s)")

        fillers = []
        if model_path:
            section("Filler Word Detection")
            fillers = detect_filler_words(input_file, model_path, silences)
            status(f"found {YELLOW}{len(fillers)}{RESET} filler region(s)")

        all_regions = merge_regions(silences, fillers)
        segments = regions_to_segments(all_regions, total, args.duration)

        if not all_regions:
            status(f"{DIM}nothing to remove, skipping{RESET}")
            continue

        kept = sum(seg.end - seg.start for seg in segments)
        removed = total - kept
        pct = (removed / total * 100) if total > 0 else 0
        status(f"segments to keep: {BOLD}{len(segments)}{RESET}")
        status(
            f"removing {YELLOW}{removed:.1f}s{RESET} of {total:.1f}s "
            f"({BOLD}{pct:.1f}%{RESET})"
        )
        for i, (start, end) in enumerate(all_regions, 1):
            ts_start = format_timestamp(start)
            ts_end = format_timestamp(end)
            log.debug("%d. %s -> %s (%.1fs)", i, ts_start, ts_end, end - start)

        section("Encoding")
        built = build_command(
            input_file,
            output_file,
            segments,
            gpu,
            args.codec,
            args.fade,
        )
        if built is None:
            continue

        if isinstance(built, subprocess.CompletedProcess):
            result = built
        else:
            log.debug("encode cmd: %s", " ".join(built))
            status(f"writing {MAGENTA}{output_file}{RESET}")
            try:
                result = subprocess.run(built)
            except KeyboardInterrupt:
                error("interrupted, cleaning up...")
                if output_file.exists():
                    output_file.unlink()
                    status(f"removed partial {output_file}")
                sys.exit(130)

        if result.returncode != 0:
            error(f"ffmpeg exited with code {result.returncode}")
            continue

        section("Done")
        status(f"{input_file.name}")
        parts = []
        if silences:
            parts.append(f"{YELLOW}{len(silences)}{RESET} silence(s)")
        if fillers:
            parts.append(f"{YELLOW}{len(fillers)}{RESET} filler(s)")
        if parts:
            status(f"detected {' and '.join(parts)}")
        status(
            f"trimmed {YELLOW}{removed:.1f}s{RESET} ({BOLD}{pct:.1f}%{RESET}), "
            f"{format_timestamp(total)} down to {GREEN}{format_timestamp(kept)}{RESET}"
        )
        status(f"{MAGENTA}{output_file}{RESET}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nInterrupted", file=sys.stderr)
        sys.exit(130)
